{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"661eaf9d","executionInfo":{"status":"ok","timestamp":1662523322326,"user_tz":-330,"elapsed":35944,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}},"outputId":"5c7ba190-bafe-46ba-ff0a-26ef5a042690"},"source":["import soundfile # to read audio file\n","import numpy as np\n","import librosa # to extract speech features\n","import glob\n","import os\n","import pickle # to save model after training\n","from google.colab import drive # to access google drive to use dataset\n","drive.mount('/content/drive')\n","from sklearn.model_selection import train_test_split # for splitting training and testing\n","from sklearn.neural_network import MLPClassifier # multi-layer perceptron model\n","from sklearn.metrics import accuracy_score # to measure accuracy of model"],"id":"661eaf9d","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"d5165db4","executionInfo":{"status":"ok","timestamp":1662523322327,"user_tz":-330,"elapsed":11,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}}},"source":["def extract_feature(file_name, **kwargs):\n","    \"\"\"\n","    Extract feature from audio file `file_name`\n","        Features supported:\n","            - MFCC (mfcc)\n","            - Chroma (chroma)\n","            - MEL Spectrogram Frequency (mel)\n","            - Contrast (contrast)\n","            - Tonnetz (tonnetz)\n","        e.g:\n","        `features = extract_feature(path, mel=True, mfcc=True)`\n","    \"\"\"\n","    mfcc = kwargs.get(\"mfcc\")\n","    chroma = kwargs.get(\"chroma\")\n","    mel = kwargs.get(\"mel\")\n","    contrast = kwargs.get(\"contrast\")\n","    tonnetz = kwargs.get(\"tonnetz\")\n","    with soundfile.SoundFile(file_name) as sound_file:\n","        X = sound_file.read(dtype=\"float32\")\n","        sample_rate = sound_file.samplerate\n","        if chroma or contrast:\n","            stft = np.abs(librosa.stft(X))\n","        result = np.array([])\n","        if mfcc:\n","            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n","            result = np.hstack((result, mfccs))\n","        if chroma:\n","            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n","            result = np.hstack((result, chroma))\n","        if mel:\n","            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n","            result = np.hstack((result, mel))\n","        if contrast:\n","            contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n","            result = np.hstack((result, contrast))\n","        if tonnetz:\n","            tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n","            result = np.hstack((result, tonnetz))\n","    return result"],"id":"d5165db4","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"94e6f817","executionInfo":{"status":"ok","timestamp":1662523322327,"user_tz":-330,"elapsed":8,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}}},"source":["# all emotions on RAVDESS dataset\n","int2emotion = {\n","    \"01\": \"neutral\",\n","    \"02\": \"calm\",\n","    \"03\": \"happy\",\n","    \"04\": \"sad\",\n","    \"05\": \"angry\",\n","    \"06\": \"fearful\",\n","    \"07\": \"disgust\",\n","    \"08\": \"surprised\"\n","}\n","\n","# we allow only these emotions\n","AVAILABLE_EMOTIONS = {\n","    \"angry\",\n","    \"sad\",\n","    \"neutral\",\n","    \"happy\"\n","}\n","\n","def load_data(test_size=0.2):\n","    X, y = [], []\n","    for file in glob.glob(\"/content/drive/MyDrive/Colab Notebooks/RAVDESS_Emotional_speech_audio/speech-emotion-recognition-ravdess-data/Actor_*/*.wav\"):\n","        # get the base name of the audio file\n","        basename = os.path.basename(file)\n","        # get the emotion label\n","        emotion = int2emotion[basename.split(\"-\")[2]]\n","        # Allows only AVAILABLE_EMOTIONS we set\n","        if emotion not in AVAILABLE_EMOTIONS:\n","            continue\n","        # extract speech features\n","        features = extract_feature(file, mfcc=True, chroma=True, mel=True)\n","        # add to data\n","        X.append(features)\n","        y.append(emotion)\n","    # split the data to training and testing and return it\n","    return train_test_split(np.array(X), y, test_size=test_size, random_state=7)"],"id":"94e6f817","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8c4f1d2","executionInfo":{"status":"ok","timestamp":1662523795021,"user_tz":-330,"elapsed":472701,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}}},"source":["# load RAVDESS dataset, 75% training 25% testing\n","X_train, X_test, y_train, y_test = load_data(test_size=0.25)"],"id":"f8c4f1d2","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"5324e598","executionInfo":{"status":"ok","timestamp":1662523795024,"user_tz":-330,"elapsed":27,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d563a40-e1f0-4b5d-f641-4d2c465e3220"},"source":["# print some details\n","# number of samples in training data\n","print(\"[+] Number of training samples:\", X_train.shape[0])\n","# number of samples in testing data\n","print(\"[+] Number of testing samples:\", X_test.shape[0])\n","# number of features used\n","# this is a vector of features extracted \n","# using extract_features() function\n","print(\"[+] Number of features:\", X_train.shape[1])"],"id":"5324e598","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[+] Number of training samples: 505\n","[+] Number of testing samples: 169\n","[+] Number of features: 180\n"]}]},{"cell_type":"code","metadata":{"id":"dc0423a5","executionInfo":{"status":"ok","timestamp":1662523795026,"user_tz":-330,"elapsed":19,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}}},"source":["# best model, determined by a grid search\n","model_params = {\n","    'alpha': 0.01,\n","    'batch_size': 256,\n","    'epsilon': 1e-08, \n","    'hidden_layer_sizes': (300,), \n","    'learning_rate': 'adaptive', \n","    'max_iter': 500, \n","}"],"id":"dc0423a5","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"1b8fe9d7","executionInfo":{"status":"ok","timestamp":1662523795027,"user_tz":-330,"elapsed":19,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}}},"source":["# initialize Multi Layer Perceptron classifier\n","# with best parameters\n","model = MLPClassifier(**model_params)"],"id":"1b8fe9d7","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"f515f88b","executionInfo":{"status":"ok","timestamp":1662523798363,"user_tz":-330,"elapsed":3355,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d548281f-9bae-4cb1-9a33-44bb759e4979"},"source":["# train the model\n","print(\"[*] Training the model...\")\n","model.fit(X_train, y_train)"],"id":"f515f88b","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[*] Training the model...\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n","              learning_rate='adaptive', max_iter=500)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"e5f5137f","executionInfo":{"status":"ok","timestamp":1662523798366,"user_tz":-330,"elapsed":21,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7045620f-f8ce-49e8-fc16-0c445ae4f69a"},"source":["# predict 25% of data to measure how good the model is\n","y_pred = model.predict(X_test)\n","\n","# calculate the accuracy\n","accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n","\n","print(\"Accuracy: {:.2f}%\".format(accuracy*100))"],"id":"e5f5137f","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 68.05%\n"]}]},{"cell_type":"code","metadata":{"id":"148de5de","executionInfo":{"status":"ok","timestamp":1662523798367,"user_tz":-330,"elapsed":17,"user":{"displayName":"Priyam Agarwal","userId":"17529728817105614039"}}},"source":["# saving the model\n","# make result directory\n","if not os.path.isdir(\"result\"):\n","    os.mkdir(\"result\")\n","\n","pickle.dump(model, open(\"result/mlp_classifier.model\", \"wb\"))"],"id":"148de5de","execution_count":10,"outputs":[]}]}